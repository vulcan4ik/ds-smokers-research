














# помощник писать код
import jupyter_black
jupyter_black.load()

# импорты основных библиотек
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from scipy import stats
import sklearn.metrics as metrics

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score, precision_recall_curve
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import (
    classification_report,
    PrecisionRecallDisplay,
    RocCurveDisplay,
)

from catboost import CatBoostClassifier
from catboost import Pool


# для анализа зависимостей между признаками
import phik
from phik.report import plot_correlation_matrix

import warnings


# поможет раскрасить датафрейм
from matplotlib.colors import LinearSegmentedColormap


# Скроем лишние предупреждения
warnings.filterwarnings("ignore")
# Настройки Pandas для вывода всех столбцов
pd.set_option("display.max_columns", None)
# улучшение четкости изображения графиков
get_ipython().run_line_magic("config", " InlineBackend.figure_format = 'retina'")


get_ipython().getoutput("gdown 1XQ7LqTk9uP4lbu4wR9w0KnKF8qvv2XsE")


get_ipython().getoutput("unzip leopard-challenge-classification.zip")


# чтениек датасета
try:
    train = pd.read_csv("/home/jovyan/work/data/train.csv")
except:
    train = pd.read_csv("../data/train.csv")


train.head(10)


train.info()


train.columns = train.columns.str.lower()


try:
    test = pd.read_csv("/home/jovyan/work/data/test.csv")
except:
    test = pd.read_csv("../data/test.csv")


test.info()


test.columns = test.columns.str.lower()


train = train.rename(
    columns={
        "fasting blood sugar": "fasting_blood_sugar",
        "urine protein": "urine_protein",
        "serum creatinine": "serum_creatinine",
        "dental caries": "dental_caries",
    }
)
test = test.rename(
    columns={
        "fasting blood sugar": "fasting_blood_sugar",
        "urine protein": "urine_protein",
        "serum creatinine": "serum_creatinine",
        "dental caries": "dental_caries",
    }
)


cols = train.columns.to_list()

for col in cols:
    print(f"Признак: {col}, пропуски: {train[col].isnull().sum()}")








display(train["oral"].value_counts())
test["oral"].value_counts()





train["tartar"] = train["tartar"].replace({"Y": 1, "N": 0})
test["tartar"] = test["tartar"].replace({"Y": 1, "N": 0})


train.info()





def count_outliers(df):
    """
    Функция анализирует выбросы в числовых столбцах DataFrame,
    используя метод межквартильного размаха (IQR).
    Возвращает DataFrame с количеством и процентом выбросов.
    """
    numeric_cols = df.select_dtypes(include=["number"]).columns

    features, lower_outliers, upper_outliers = [], [], []

    for col in numeric_cols:
        q1, q3 = df[col].quantile([0.25, 0.75])
        iqr = q3 - q1
        lower_limit, upper_limit = q1 - 1.5 * iqr, q3 + 1.5 * iqr

        lower_count = (df[col] < lower_limit).sum()
        upper_count = (df[col] > upper_limit).sum()

        if lower_count > 0 or upper_count > 0:
            features.append(col)
            lower_outliers.append(lower_count)
            upper_outliers.append(upper_count)

    if features:
        return pd.DataFrame(
            {
                "variables": features,
                "lower_outliers": lower_outliers,
                "upper_outliers": upper_outliers,
                "lower_outliers%": [
                    round(x / len(df) * 100, 2) for x in lower_outliers
                ],
                "upper_outliers%": [
                    round(x / len(df) * 100, 2) for x in upper_outliers
                ],
            }
        )

    return None


print("Число дубликатов наблюдений: ", train.duplicated().sum())
len(train)


count_outliers(train)


# серый стиль оформления и убираем предупреждения
sns.set_style("darkgrid")
sns.set_palette("twilight_r")

# Игнорировать все предупреждения
warnings.filterwarnings("ignore")


plt.style.use("seaborn-v0_8-darkgrid")
sns.set_palette("twilight_r")
for col in ["age", "height(cm)", "weight(kg)", "waist(cm)"]:
    plt.figure(figsize=(8, 4))
    sns.boxplot(x=train[col], medianprops={"color": "yellow", "linewidth": 2})
    plt.title(f"Диаграмма размаха {col}")
    plt.xlabel(col)
    plt.xticks(rotation=45)
    plt.show()





train["smoking"].value_counts(normalize=True)


train["dental_caries"].value_counts()





fig, ax = plt.subplots(1, 2, figsize=(14, 6))

sns.countplot(x=train["smoking"], ax=ax[0])
train["smoking"].value_counts().plot(kind="pie", autopct="%1.1f%%", ax=ax[1])
plt.show()

smoke_1 = train[train["smoking"] == 1].shape[0]
smoke_0 = train[train["smoking"] == 0].shape[0]


display("Курят: ", smoke_1)
display("Не курят: ", smoke_0)








import phik


phik_overview = train.phik_matrix()


phik_overview["smoking"].drop("smoking").sort_values(ascending=False).to_frame()


plt.figure(figsize=(8, 6))
sns.heatmap(
    phik_overview[["smoking"]]
    .drop("smoking")
    .sort_values(by="smoking", ascending=False),
    annot=True,
    cmap="Blues",
)
plt.title("Корреляция φk с признаком 'smoking'")
plt.show()











train["age"].hist(bins=12)
plt.xlabel("Age");


plt.figure(figsize=(10, 6))
sns.histplot(train["age"], kde=True, color="violet", stat="density", alpha=0.5)
plt.xlabel("Возраст")
plt.ylabel("Плотность результатов")
plt.title("Распределение возраста на обучающем наборе")
plt.show()


train["age"].describe()





# создадим отдельно датасеты курящих и некурящих
df_smokers = train[train["smoking"] == 1]
df_non_smokers = train[train["smoking"] == 0]
age_smoking = df_smokers.groupby("age")["smoking"].agg(["count"])
age_non_smoking = df_non_smokers.groupby("age")["smoking"].agg(["count"])
print("Распределение курящих по возрастам")
age_smoking


fig, ax = plt.subplots(1, 2, figsize=(15, 5))

age_smoking["count"].plot(kind="bar", rot=0, ax=ax[0])
ax[0].set_title("Распределение курящих людей по возрасту")
ax[0].set_xlabel("Возраст")
ax[0].set_ylabel("Количество")

age_non_smoking["count"].plot(kind="bar", rot=0, ax=ax[1], color="skyblue")
ax[1].set_title("Распределение некурящих людей по возрасту")
ax[1].set_xlabel("Возраст")
ax[1].set_ylabel("Количество")

plt.tight_layout()
plt.show()





plt.figure(figsize=(10, 6))
sns.kdeplot(df_smokers["age"], label="Курящие", fill=True, color="red")
sns.kdeplot(df_non_smokers["age"], label="Некурящие", fill=True, color="blue")
plt.title("Распределение возраста среди курящих и некурящих")
plt.xlabel("Возраст")
plt.ylabel("Плотность распределения")
plt.legend()
plt.show()





smokers_40 = (
    (age_smoking[age_smoking.index == 40]["count"].values[0])
    / train["smoking"].sum()
    * 100
)
smokers_40
print(f"Доля курильщиков в возрасте 40 лет от общего числа курящих: {smokers_40:.2f}%")


age_smoking["mean"] = (age_smoking["count"] / age_smoking["count"].sum()).round(2)
age_smoking.sort_values(by="mean", ascending=False)
age_smoking_sorted = age_smoking.sort_values(by="mean", ascending=False)

age_smoking_sorted.style.background_gradient(cmap="coolwarm", subset=["mean"])











plt.figure(figsize=(8, 6))
stats.probplot(train["height(cm)"], dist="norm", plot=plt)
plt.title("Q-Q график для нормальности данных")
plt.xlabel("Теоретические квантильные значения")
plt.ylabel("Реальные квантильные значения height(cm)")
plt.show()


plt.figure(figsize=(8, 6))
stats.probplot(train["waist(cm)"], dist="norm", plot=plt)
plt.title("Q-Q график для нормальности данных")
plt.xlabel("Теоретические квантильные значения ")
plt.ylabel("Реальные квантильные значения waist(cm)")
plt.show()


plt.figure(figsize=(8, 6))
stats.probplot(train["weight(kg)"], dist="norm", plot=plt)
plt.title("Q-Q график для нормальности данных")
plt.xlabel("Теоретические квантильные значения")
plt.ylabel("Реальные квантильные значения weight(kg)")
plt.show()





plt.figure(figsize=(12, 6))
sns.histplot(
    train["height(cm)"],
    kde=True,
    color="violet",
    stat="density",
    label="height",
    alpha=0.5,
)
plt.xticks()
plt.xlabel("рост, см")
plt.ylabel(
    "Плотность результатов",
)
plt.title("Распределение роста")
plt.legend()
plt.show()


plt.figure(figsize=(12, 6))
sns.histplot(
    train["weight(kg)"],
    kde=True,
    color="magenta",
    stat="density",
    label="weight",
    alpha=0.5,
)
plt.xticks(range(40, 140, 5))
plt.xlabel("вес, кг")
plt.ylabel(
    "Плотность результатов",
)
plt.title("Распределение веса")
plt.legend()
plt.show()


plt.figure(figsize=(12, 6))
sns.histplot(
    train["waist(cm)"],
    kde=True,
    color="skyblue",
    stat="density",
    label="waist",
    alpha=0.5,
)
# plt.xticks(range(40,130,5))
plt.xlabel("талия, см")
plt.ylabel(
    "Плотность результатов",
)
plt.title("Распределение объема талии")
plt.legend()
plt.show()


train["height(cm)"].value_counts()





for col in ["height(cm)", "weight(kg)", "waist(cm)"]:
    print(f"Среднее значение {col}: {train[col].mean().round(2)}")





from scipy.stats import normaltest

for col in ["height(cm)", "weight(kg)", "waist(cm)"]:
    stat, p = normaltest(train[col])
    print(f"{col}: статистика={stat:.2f}, p-value={p:.5f}")

    if p < 0.05:
        print(f"⚠️ {col}: Данные НЕ распределены нормально (p < 0.05)")
    else:
        print(f"✅ {col}: Данные можно считать нормально распределёнными (p >= 0.05)")











med_features = [
    "eyesight(left)",
    "eyesight(right)",
    "hearing(left)",
    "hearing(right)",
    "systolic",
    "relaxation",
    "fasting_blood_sugar",
    "cholesterol",
    "triglyceride",
    "hdl",
    "ldl",
    "hemoglobin",
    "urine_protein",
    "serum_creatinine",
    "ast",
    "alt",
    "gtp",
]


def investigate_column(column):
    """
    функция принимает значение столбца df и выводит его описание для анализа,
    считает пропуски, уникальные значения ,
    тип данных и строит диаграмму размаха для числовых типов данных
    """

    print(column.name)
    print("Кол-во Null и Na значений:", column.isna().sum())
    print("Кол-во уникальных значений:", column.nunique())
    print("Data type:", column.dtype)

    # Описание статистики
    print(column.describe())

    # Частота уникальных значений
    print("Частота уникальных значений столбца:\n", column.value_counts())

    # Минимальное и максимальное значение (для числовых данных)
    if pd.api.types.is_numeric_dtype(column) and not pd.api.types.is_bool_dtype(column):
        print("Minimum:", column.min())
        print("Maximum:", column.max())

        # Построение boxplot
        plt.figure(figsize=(10, 6))
        # Удаляем NaN для отображения и разворачиваем график
        plt.boxplot(column.dropna(), vert=False)
        plt.title(f"Диаграмма размаха для столбца {column.name}")
        plt.xlabel("Значения")
        plt.show()


for col in med_features:
    investigate_column(train[col])











plt.figure(figsize=(10, 5))
sns.histplot(
    df_smokers["hemoglobin"],
    bins=100,
    kde=False,
    color="red",
    alpha=0.5,
    label="Курящие",
)
sns.histplot(
    df_non_smokers["hemoglobin"],
    bins=100,
    kde=False,
    color="green",
    alpha=0.5,
    label="Некурящие",
)
plt.xlabel("Уровень гемоглобина")
plt.ylabel("Частота")
plt.title("Распределение уровня гемоглобина у курящих и некурящих")
plt.legend()
plt.grid(True)
plt.show()


df_smokers["hemoglobin"].mean()


df_non_smokers["hemoglobin"].mean()








train["triglyceride"].hist();





train["triglyceride_group"] = pd.cut(
    train["triglyceride"], bins=[0, 50, 100, 150, 200, 250, 300, 350, 400]
)


t = train.groupby("triglyceride_group")["smoking"].value_counts(normalize=True)
t.unstack().style.background_gradient(cmap="coolwarm")


t = t.reset_index()
t["proportion"] = pd.to_numeric(t["proportion"], errors="coerce")
t["triglyceride_group"] = t["triglyceride_group"].astype("category")
t


t["triglyceride_group"] = t["triglyceride_group"].astype(str)
plt.figure(figsize=(10, 6))
sns.lineplot(
    x="triglyceride_group",
    y="proportion",
    hue="smoking",  # Разделение линий по статусу курения
    data=t,
    marker="o",
    style="smoking",
    palette="Set2",
)
plt.title("Процент курящих в зависимости от показателя триглицеридов")
plt.xlabel("Диапазон триглицеридов")
plt.ylabel("Процент курящих")
plt.xticks(rotation=0)
plt.legend(title="Курение")
plt.tight_layout()
plt.show()








sns.boxplot(x="gtp", data=train[train["gtp"] < 400])
plt.show()


train["gtp"].describe()





train["gtp_group"] = pd.cut(train["gtp"], bins=np.linspace(0, 1000, 10), precision=0)
train.groupby("gtp_group")["smoking"].value_counts()


d = train.groupby("gtp_group")["smoking"].agg(["count", "mean"])
d.columns = ["count", "part_smokers"]
d.sort_values(by="part_smokers", ascending=False).style.background_gradient(
    cmap="coolwarm", subset="part_smokers"
)





d.reset_index().plot(
    x="gtp_group",
    y="part_smokers",
    kind="line",
    marker="o",
    figsize=(8, 5),
    legend=False,
    rot=45,
)
plt.xlabel("диапазон gtp")
plt.ylabel("Доля курящих")
plt.title("Доля курящих по группам GTP")
plt.grid(True)
plt.show()








train.groupby("dental_caries")["smoking"].value_counts(normalize=True)





train.groupby("smoking")["dental_caries"].value_counts(normalize=True)








train.groupby("smoking")["tartar"].value_counts(normalize=True).unstack()


# Оценка относительного риска (Odds Ratio)
odds_smokers = train[train["smoking"] == 1]["tartar"].mean() / (
    1 - train[train["smoking"] == 1]["tartar"].mean()
)
odds_non_smokers = train[train["smoking"] == 0]["tartar"].mean() / (
    1 - train[train["smoking"] == 0]["tartar"].mean()
)
odds_ratio = odds_smokers / odds_non_smokers
print(f"Отношение шансов (OR): {odds_ratio:.2f}")








plt.boxplot(train["systolic"], vert=False)
plt.show()


plt.figure(figsize=(10, 5))
sns.histplot(
    df_smokers["systolic"],
    bins=100,
    kde=False,
    color="red",
    alpha=0.5,
    label="Курящие",
)
sns.histplot(
    df_non_smokers["systolic"],
    bins=100,
    kde=False,
    color="green",
    alpha=0.5,
    label="Некурящие",
)
plt.xlabel("Систолическое давление")
plt.ylabel("Частота")
plt.title("Распределение уровня гемоглобина у курящих и некурящих")
plt.legend()
plt.grid(True)
plt.show()





smokers_mean_sys, smokers_med_sys = df_smokers["systolic"].agg(["mean", "median"])


non_smokers_mean_sys, non_smokers_med_sys = df_non_smokers["systolic"].agg(
    ["mean", "median"]
)


print(
    f"""Курящие:
Среднее значение systolic = {smokers_mean_sys:.2f}
Медианное значение = {smokers_med_sys:.2f}

Некурящие:
Среднее значение systolic = {non_smokers_mean_sys:.2f}
Медианное значение = {non_smokers_med_sys:.2f}"""
)





train.columns


# разобьем на возрастные группы
train["age_group"] = pd.cut(
    train["age"],
    bins=[18, 30, 40, 50, 60, 100],
    labels=["18 -30", "30-40", "40-50", "50-60", "60+"],
)

# Сравнить systolic в разных возрастных группах
for age_group in train["age_group"].unique():
    subset = train[train["age_group"] == age_group]
    smokers_mean = subset[subset["smoking"] == 1]["systolic"].mean()
    non_smokers_mean = subset[subset["smoking"] == 0]["systolic"].mean()
    diff = smokers_mean - non_smokers_mean

    print(f"Возраст {age_group}: разница = {diff:.2f}")








plt.boxplot(train["systolic"], vert=False)
plt.show()





train = train[train["systolic"] < 225]
len(train)


plt.boxplot(train["relaxation"], vert=False)
plt.show()


train["relaxation"].hist();


med_features


plt.boxplot(train["fasting_blood_sugar"], vert=False);


train = train[train["fasting_blood_sugar"] < 400]
len(train)


plt.boxplot(train["cholesterol"], vert=False)
plt.show()


train = train[train["cholesterol"] < 400]
len(train)


plt.boxplot(train["triglyceride"], vert=False);





train = train[train["triglyceride"] < 400]
len(train)


plt.boxplot(train["hdl"], vert=False);


train = train[train["hdl"] < 150]
len(train)


plt.boxplot(train["ldl"], vert=False);


# оставим в выборке
train[train["ldl"] > 800]


plt.boxplot(train["hemoglobin"], vert=False);


plt.boxplot(train["serum_creatinine"], vert=False)
plt.show()


train[train["serum_creatinine"] > 5]


plt.boxplot(train["ast"], vert=False)
plt.show()


train[train["ast"] > 400]


plt.boxplot(train["gtp"], vert=False);


train[train["gtp"] > 600]["smoking"].value_counts(normalize=True)





plt.boxplot(train["alt"], vert=False);


train[train["alt"] > 500]


train.reset_index()


train_cleaned = train.drop(
    columns=["id", "oral", "triglyceride_group", "gtp_group", "age_group"]
)
train_raw = pd.read_csv("../data/train.csv")
del_percentage = (len(train_raw) - len(train_cleaned)) / len(train_raw)
print(
    f"""
        Размер начального датасета {len(train_raw)} строк,
        финальный размер {len(train_cleaned)}
        Удалено {len(train_raw) - len(train_cleaned)} строк что составляет {del_percentage:.2%}"""
)















































X = train_cleaned.drop(columns=["smoking"])
y = train_cleaned["smoking"]


X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.25, random_state=77, stratify=y
)


model_base = RandomForestClassifier()
model_base.fit(X_train, y_train)


y1_pred = model_base.predict(X_val)
print(classification_report(y_val, y1_pred, digits=6))


print(f"F1 метрика базовой модели : {f1_score(y_val, y1_pred)}")


fig, ax = plt.subplots(1, 2, figsize=(16, 6))
pr_disp = PrecisionRecallDisplay.from_predictions(
    y_val, y1_pred, ax=ax[0], name="Random Forest Classifier"
)
roc_disp = RocCurveDisplay.from_predictions(
    y_val, y1_pred, ax=ax[1], name="Random Forest Classifier"
)

plt.show()





train_cleaned.info()


#  оценим важность признаков
feature_importance = pd.DataFrame(
    {"Feature": X.columns, "Importance": model_base.feature_importances_}
).sort_values(by="Importance", ascending=False)
feature_importance.style.background_gradient(cmap="coolwarm")





train_cleaned.columns.to_list()


train, val = train_test_split(
    train_cleaned, train_size=0.75, random_state=42, stratify=train_cleaned["smoking"]
)


X = [
    "age",
    "waist(cm)",
    "systolic",
    "relaxation",
    "fasting_blood_sugar",
    "cholesterol",
    "triglyceride",
    "hdl",
    "ldl",
    "hemoglobin",
    "urine_protein",
    "ast",
    "alt",
    "gtp",
]
y = "smoking"
cat_features = ["age"]


train_data = Pool(data=train[X], label=train[y], cat_features=cat_features)

valid_data = Pool(data=val[X], label=val[y], cat_features=cat_features)


params = {"random_seed": 42, "verbose": 100, "learning_rate": 0.017}


model = CatBoostClassifier(**params)

model.fit(train_data, eval_set=valid_data)


y_pred = model.predict(val[X])
y1_score = model.predict_proba(val[X])[:, 1]


def print_metrics(y_true, y_pred):
    """
    Вывод основных метрик модели
    и кривой ROC AUC

    """
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    roc_auc = roc_auc_score(y_true, y1_score)

    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-score: {f1:.4f}")
    print(f"ROC AUC: {roc_auc:.4f}")


print_metrics(val["smoking"], y_pred)





feature_importance_catboost = pd.DataFrame(
    {
        "Feature": X,
        "Importance": model.get_feature_importance(
            Pool(data=val[X], label=val[y], cat_features=cat_features)
        ),
    }
).sort_values(by="Importance", ascending=False)
feature_importance_catboost


train_cleaned["smoking"].value_counts(normalize=True)


from collections import Counter

class_counts = Counter(train["smoking"])
weight_0 = class_counts[1] / class_counts[0]
weight_1 = class_counts[0] / class_counts[1]
display(weight_0)
display(weight_1)





X = [
    "age",
    "waist(cm)",
    "triglyceride",
    "ldl",
    "hemoglobin",
    "alt",
    "gtp",
    "dental_caries",
    "tartar",
]
y = "smoking"
cat_features = ["dental_caries", "tartar"]


train_data = Pool(data=train[X], label=train[y], cat_features=cat_features)

valid_data = Pool(data=val[X], label=val[y], cat_features=cat_features)


params = {
    "random_seed": 42,
    "verbose": 100,
    "learning_rate": 0.009,
    "class_weights": [weight_0, weight_1],
}


model = CatBoostClassifier(**params)


model.fit(train_data, eval_set=valid_data)


y2_pred = model.predict(val[X])
y2_score = model.predict_proba(val[X])[:, 1]
print_metrics(val[y], y2_pred)


import sklearn.metrics as metrics

# построим график ROC
fpr, tpr, threshold = metrics.roc_curve(val[y], y2_score)
roc_auc = metrics.auc(fpr, tpr)

# method I: plt
import matplotlib.pyplot as plt

plt.title("Receiver Operating Characteristic")
plt.plot(fpr, tpr, "b", label="AUC = %0.2f" % roc_auc)
plt.legend(loc="lower right")
plt.plot([0, 1], [0, 1], "r--")
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel("True Positive Rate")
plt.xlabel("False Positive Rate")
plt.show()





X = [
    "age",
    "waist(cm)",
    "triglyceride",
    "ldl",
    "hemoglobin",
    "alt",
    "gtp",
    "dental_caries",
    "tartar",
]
y = "smoking"
cat_features = ["age", "dental_caries", "tartar"]


train_data = Pool(data=train[X], label=train[y], cat_features=cat_features)

valid_data = Pool(data=val[X], label=val[y], cat_features=cat_features)


params = {
    "random_seed": 42,
    "verbose": 100,
    "learning_rate": 0.0079,
    "eval_metric": "AUC",
    "class_weights": [weight_0, weight_1],
}


model = CatBoostClassifier(**params)


model.fit(train_data, eval_set=valid_data)


y3_pred = model.predict(val[X])
y3_score = model.predict_proba(val[X])[:, 1]
print_metrics(val[y], y3_pred)





X = [
    "age",
    "waist(cm)",
    "triglyceride",
    "ldl",
    "hemoglobin",
    "alt",
    "gtp",
    "tartar",
    "systolic",
    "relaxation",
    "dental_caries",
]
y = "smoking"
cat_features = ["tartar", "dental_caries"]


class_weights = compute_class_weight(
    "balanced", classes=np.unique(train[y]), y=train[y]
)

print(f"Веса классов:")
print(f"  Класс 0 (не курит): {class_weights[0]:.3f}")
print(f"  Класс 1 (курит):   {class_weights[1]:.3f}")


train_data = Pool(data=train[X], label=train[y], cat_features=cat_features)

valid_data = Pool(data=val[X], label=val[y], cat_features=cat_features)


# обновим параметры
params = {
    "iterations": 1200,
    "learning_rate": 0.0075,
    "depth": 6,
    "class_weights": class_weights,
    "verbose": 100,
    "eval_metric": "AUC",
    "early_stopping_rounds": 50,
    "random_state": 42,
}

model = CatBoostClassifier(**params)


model.fit(train_data, eval_set=valid_data)





y_proba = model.predict_proba(val[X])[:, 1]


# вычислим precision-recall для разных порогов
precision, recall, thresholds = precision_recall_curve(val[y], y_proba)

#  F1 для каждого порога
f1_scores_by_thresh = (
    2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1] + 1e-10)
)

# Найти лучший порог
best_idx = np.argmax(f1_scores_by_thresh)
best_thresh = thresholds[best_idx]
best_f1 = f1_scores_by_thresh[best_idx]

print(f"\nОптимальный порог: {best_thresh:.3f}")
print(f"F1-score: {best_f1:.4f}")


y4_pred = (y_proba > best_thresh).astype(int)


print("\nИтоговые метрики (с оптимальным порогом):")
print_metrics(val[y], y4_pred)





feature_importance = pd.DataFrame(
    {"feature": X, "importance": model.get_feature_importance()}
).sort_values("importance", ascending=False)

print(feature_importance)

#  признаки с важностью < 5% от максимума
weak_features = feature_importance[feature_importance["importance"] < 5][
    "feature"
].tolist()


weak_features


X_imp = [f for f in X if f not in weak_features]


X_imp


# обновим Pool
train_data = Pool(data=train[X_imp], label=train[y])
valid_data = Pool(
    data=val[X_imp],
    label=val[y],
)


params = {
    "iterations": 1200,
    "learning_rate": 0.0075,
    "depth": 7,
    "eval_metric": "AUC",
    "class_weights": class_weights,
    "verbose": 100,
    "random_state": 42,
    "early_stopping_rounds": 100,
}


model_v2 = CatBoostClassifier(**params)
model_v2.fit(train_data, eval_set=valid_data)


y_proba_v2 = model_v2.predict_proba(val[X_imp])[:, 1]


best_thresh = 0.5
best_f1 = 0

for thresh in np.linspace(0.1, 0.9, 50):
    y_pred = (y_proba_v2 > thresh).astype(int)
    f1 = f1_score(val[y], y_pred)
    if f1 > best_f1:
        best_f1 = f1
        best_thresh = thresh

print(f"Лучший порог: {best_thresh:.3f}")
print(f"F1-score: {best_f1:.4f}")

# Применить и вывести все метрики
y_pred_final = (y_proba_v2 > best_thresh).astype(int)

print("\n" + "=" * 60)
print("СРАВНЕНИЕ")
print("=" * 60)
print(f"Было (старые параметры):  F1 = 0.4403")
print(f"Стало (новые параметры):  F1 = {best_f1:.4f}")
print(f"Улучшение: {(best_f1 - 0.4403) / 0.4403 * 100:+.1f}%")

print("\nИтоговые метрики:")
print_metrics(val[y], y_pred_final)





X = [
    "age",
    "waist(cm)",
    "triglyceride",
    "ldl",
    "hemoglobin",
    "alt",
    "gtp",
    "tartar",
    "systolic",
    "relaxation",
    "dental_caries",
]
cat_features = ["tartar", "dental_caries"]
y = "smoking"


train_data = Pool(data=train[X], label=train[y], cat_features=cat_features)

valid_data = Pool(data=val[X], label=val[y], cat_features=cat_features)


params = {
    "iterations": 1200,
    "learning_rate": 0.0075,
    "depth": 7,
    "eval_metric": "AUC",
    "class_weights": class_weights,
    "verbose": 100,
    "random_state": 42,
    "early_stopping_rounds": 100,
}


model = CatBoostClassifier(**params)
model.fit(train_data, eval_set=valid_data)


y_proba = model.predict_proba(val[X])[:, 1]
best_thresh = 0.5
best_f1 = 0

for thresh in np.linspace(0.1, 0.9, 50):
    y_pred_thresh = (y_proba > thresh).astype(int)
    f1 = f1_score(val[y], y_pred_thresh)
    if f1 > best_f1:
        best_f1 = f1
        best_thresh = thresh

print(f"Лучший порог: {best_thresh:.3f}, F1-score: {best_f1:.4f}")


y_pred = model.predict(val[X])
print_metrics(val[y], y_pred)





y_score_test = model.predict_proba(test[X])[:, 1]
test["smoking"] = (y_score_test > best_thresh).astype(int)  # применяем лучший порог
test.head(5)





X = [
    "age",
    "waist(cm)",
    "triglyceride",
    "ldl",
    "hemoglobin",
    "alt",
    "gtp",
    "tartar",
    "systolic",
    "relaxation",
    "dental_caries",
]
cat_features = ["tartar", "dental_caries"]
y = "smoking"


train_data = Pool(data=train[X], label=train[y], cat_features=cat_features)

valid_data = Pool(data=val[X], label=val[y], cat_features=cat_features)


params = {
    "iterations": 2000,
    "learning_rate": 0.0075,
    "depth": 6,
    "l2_leaf_reg": 3.0,  # мягкая регуляризация
    "random_strength": 1.0,
    "bagging_temperature": 0.3,  # лёгкий стохастический бэггинг
    "border_count": 254,  # тоньше квантизация числовых
    "eval_metric": "AUC",
    "class_weights": class_weights,
    "verbose": 100,
    "random_state": 42,
    "early_stopping_rounds": 200,
}


model = CatBoostClassifier(**params)
model.fit(train_data, eval_set=valid_data)


y_pred_proba = model.predict_proba(val[X])[:, 1]
prec, rec, thr = precision_recall_curve(val[y], y_pred_proba)
f1 = 2 * (prec * rec) / (prec + rec + 1e-9)
best_idx = np.argmax(f1[:-1])
best_thr = thr[best_idx]
best_f1 = f1[best_idx]
print(f"Лучший порог: {best_thr:.3f}, F1-score: {best_f1:.4f}")


y_pred = model.predict(val[X])
print_metrics(val[y], y_pred)


def create_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    Создаёт оптимальный набор производных признаков для прогноза курения.
    Подходит для train / val / test.
    """
    df = df.copy()

    #  индексы тела

    df["bmi"] = round(df["weight(kg)"] / ((df["height(cm)"] / 100) ** 2), 2)
    df["waist_height_ratio"] = df["waist(cm)"] / df["height(cm)"]

    # 2. Кардиоваскулярные показатели

    df["pulse_pressure"] = df["systolic"] - df["relaxation"]

    # 3. липидный профиль

    df["ldl_hdl_ratio"] = df["ldl"] / (df["hdl"] + 1)
    df["trig_hdl_ratio"] = df["triglyceride"] / (df["hdl"] + 1)
    df["atherogenic_index"] = (df["cholesterol"] - df["hdl"]) / (df["hdl"] + 1)

    # 4. Печёночные ферменты

    df["ast_alt_ratio"] = df["ast"] / (df["alt"] + 1)
    df["liver_enzyme_sum"] = df["ast"] + df["alt"] + df["gtp"]
    df["gtp_normalized"] = df["gtp"] / (df["alt"].mean() + 1e-6)

    # 5. Взаимодействие с возрастом (одно самое сильное)

    df["age_bmi_interaction"] = df["age"] * df["bmi"]

    # 6. Логарифмическая стабилизация распределинй

    df["log_triglyceride"] = np.log1p(df["triglyceride"].clip(lower=0))
    df["log_gtp"] = np.log1p(df["gtp"].clip(lower=0))

    df = df.replace([np.inf, -np.inf], np.nan)
    df = df.fillna(df.median(numeric_only=True))

    return df


train_cleaned.columns


train_extra_features = create_features(train_cleaned)


train_extra_features.columns





train, val = train_test_split(
    train_extra_features,
    train_size=0.75,
    random_state=42,
    stratify=train_extra_features["smoking"],
)


X = [
    "age",
    "waist(cm)",
    "height(cm)",
    "weight(kg)",
    "systolic",
    "relaxation",
    "triglyceride",
    "hdl",
    "ldl",
    "hemoglobin",
    "alt",
    "gtp",
    "dental_caries",
    "tartar",
    # engineered:
    "bmi",
    "waist_height_ratio",
    "pulse_pressure",
    "ldl_hdl_ratio",
    "trig_hdl_ratio",
    "atherogenic_index",
    "ast_alt_ratio",
    "liver_enzyme_sum",
    "gtp_normalized",
    "age_bmi_interaction",
    "log_gtp",
    "log_triglyceride",
]

cat_features = ["tartar", "dental_caries"]
y = "smoking"


train_data = Pool(data=train[X], label=train[y], cat_features=cat_features)

valid_data = Pool(data=val[X], label=val[y], cat_features=cat_features)


params = {
    "iterations": 2000,
    "learning_rate": 0.0075,
    "depth": 7,
    "l2_leaf_reg": 5.0,  #  регуляризация
    "random_strength": 1.0,
    "bagging_temperature": 0.3,  # лёгкий стохастический бэггинг
    "border_count": 254,  # тоньше квантизация числовых
    "eval_metric": "AUC",
    "class_weights": class_weights,
    "verbose": 100,
    "random_state": 42,
    "early_stopping_rounds": 200,
}


model = CatBoostClassifier(**params)
model.fit(train_data, eval_set=valid_data)





y_pred = model.predict(val[X])
print_metrics(val[y], y_pred)


y_score = model.predict_proba(val[X])[:, 1]
best_thresh = 0.5
best_f1 = 0

for thresh in np.linspace(0.1, 0.9, 50):
    y_pred_thresh = (y_score > thresh).astype(int)
    f1 = f1_score(val[y], y_pred_thresh)
    if f1 > best_f1:
        best_f1 = f1
        best_thresh = thresh

print(f"Лучший порог: {best_thresh:.3f}, F1-score: {best_f1:.4f}")





X = [
    "age",
    "triglyceride",
    "hdl",
    "ldl",
    "systolic",
    "hemoglobin",
    "fasting_blood_sugar",
    "serum_creatinine",
    "ast",
    "alt",
    "gtp",
    "tartar",
    # engineered:
    "age_bmi_interaction",
    "bmi",
    "waist_height_ratio",
    "trig_hdl_ratio",
    "atherogenic_index",
    "ast_alt_ratio",
    "log_gtp",
]

cat_features = ["tartar"]
y = "smoking"


params = {
    "iterations": 2500,
    "learning_rate": 0.0075,
    "depth": 7,
    "l2_leaf_reg": 5.0,  # регуляризация
    "random_strength": 2.0,
    "bagging_temperature": 0.3,  # лёгкий стохастический бэггинг
    "border_count": 254,  # тоньше квантизация числовых
    "auto_class_weights": "Balanced",
    "verbose": 200,
    "eval_metric": "AUC",
    "random_state": 42,
    "early_stopping_rounds": 200,
}


train_data = Pool(data=train[X], label=train[y], cat_features=cat_features)

valid_data = Pool(data=val[X], label=val[y], cat_features=cat_features)


model = CatBoostClassifier(**params)
model.fit(train_data, eval_set=valid_data)


y_pred = model.predict(val[X])
print_metrics(val[y], y_pred)


y_score = model.predict_proba(val[X])[:, 1]
prec, rec, thr = precision_recall_curve(val[y], y_score)
f1 = 2 * (prec * rec) / (prec + rec + 1e-9)
best_idx = f1[:-1].argmax()

best_thr = thr[best_idx]
best_f1 = f1[best_idx]
print(f"Лучший порог: {best_thr:.3f}, F1: {best_f1:.4f}")


feature_importance_catboost = pd.DataFrame(
    {
        "Feature": X,
        "Importance": model.get_feature_importance(
            Pool(data=val[X], label=val[y], cat_features=cat_features)
        ),
    }
).sort_values(by="Importance", ascending=False)
feature_importance_catboost








def add_features_1(df):
    df = df.copy()
    # 1) Пульсовое давление
    df["PulsePressure"] = df["systolic"] - df["relaxation"]
    # 2) Соотношение ALT к GTP
    df["ALT_GTP_ratio"] = df["alt"] / (df["gtp"] + 1e-6)
    # 3) LDL к HDL
    df["LDL_to_HDL"] = df["ldl"] / (df["hdl"] + 1e-6)

    # санитария
    num = df.select_dtypes(include=["number"])
    df[num.columns] = num.replace([np.inf, -np.inf], np.nan)
    df[num.columns] = df[num.columns].fillna(df[num.columns].median())
    return df


train_extra_features_v2 = add_features_1(train_extra_features)


train, val = train_test_split(
    train_extra_features_v2,
    train_size=0.75,
    random_state=42,
    stratify=train_extra_features_v2["smoking"],
)


X = [
    # базовые:
    "age",
    "triglyceride",
    "hdl",
    "ldl",
    "systolic",
    "hemoglobin",
    "fasting_blood_sugar",
    "serum_creatinine",
    "ast",
    "gtp",
    "tartar",
    # engineered:
    "bmi",
    "waist_height_ratio",
    "trig_hdl_ratio",
    "atherogenic_index",
    "ast_alt_ratio",
    "log_gtp",
    "age_bmi_interaction",
    # новые:
    "PulsePressure",
    "ALT_GTP_ratio",
    "LDL_to_HDL",
]
cat_features = ["tartar"]
y = "smoking"


train_data = Pool(data=train[X], label=train[y], cat_features=cat_features)

valid_data = Pool(data=val[X], label=val[y], cat_features=cat_features)


params = {
    "iterations": 2500,
    "learning_rate": 0.0075,
    "depth": 7,
    "l2_leaf_reg": 5.0,  # регуляризация
    "random_strength": 2.0,
    "bagging_temperature": 0.3,  # лёгкий стохастический бэггинг
    "border_count": 254,  # тоньше квантизация числовых
    "auto_class_weights": "Balanced",
    "verbose": 200,
    "eval_metric": "AUC",
    "random_state": 42,
    "early_stopping_rounds": 200,
}


model = CatBoostClassifier(**params)
model.fit(train_data, eval_set=valid_data)


y_pred = model.predict(val[X])
print_metrics(val[y], y_pred)


feature_importance_catboost = pd.DataFrame(
    {
        "Feature": X,
        "Importance": model.get_feature_importance(
            Pool(data=val[X], label=val[y], cat_features=cat_features)
        ),
    }
).sort_values(by="Importance", ascending=False)
feature_importance_catboost


X = [
    # базовые:
    "age",
    "triglyceride",
    "hdl",
    "ldl",
    "systolic",
    "hemoglobin",
    "fasting_blood_sugar",
    "serum_creatinine",
    "ast",
    "gtp",
    "tartar",
    # engineered:
    "bmi",
    "waist_height_ratio",
    "trig_hdl_ratio",
    "atherogenic_index",
    "ast_alt_ratio",
    "log_gtp",
    "age_bmi_interaction",
    # новые:
    "LDL_to_HDL",
    "ALT_GTP_ratio",
]
cat_features = ["tartar"]
y = "smoking"


params = {
    "iterations": 2500,
    "learning_rate": 0.0075,
    "depth": 7,
    "l2_leaf_reg": 5.0,  # регуляризация
    "random_strength": 2.0,
    "bagging_temperature": 0.3,  # лёгкий стохастический бэггинг
    "border_count": 254,  # тоньше квантизация числовых
    "auto_class_weights": "Balanced",
    "verbose": 200,
    "eval_metric": "AUC",
    "random_state": 42,
    "early_stopping_rounds": 200,
}


train_data = Pool(data=train[X], label=train[y], cat_features=cat_features)

valid_data = Pool(data=val[X], label=val[y], cat_features=cat_features)


model = CatBoostClassifier(**params)
model.fit(train_data, eval_set=valid_data)


y_pred = model.predict(val[X])
print_metrics(val[y], y_pred)


y_score = model.predict_proba(val[X])[:, 1]
prec, rec, thr = precision_recall_curve(val[y], y_score)
f1 = 2 * (prec * rec) / (prec + rec + 1e-9)
best_idx = f1[:-1].argmax()

best_thr = thr[best_idx]
best_f1 = f1[best_idx]
print(f"Лучший порог: {best_thr:.3f}, F1: {best_f1:.4f}")











train, val = train_test_split(
    train_extra_features,
    train_size=0.75,
    random_state=42,
    stratify=train_extra_features["smoking"],
)


X = [
    "age",
    "triglyceride",
    "hdl",
    "ldl",
    "systolic",
    "hemoglobin",
    "fasting_blood_sugar",
    "serum_creatinine",
    "ast",
    "alt",
    "gtp",
    "tartar",
    # engineered:
    "age_bmi_interaction",
    "bmi",
    "waist_height_ratio",
    "trig_hdl_ratio",
    "atherogenic_index",
    "ast_alt_ratio",
    "log_gtp",
]

cat_features = ["tartar"]
y = "smoking"


# итоговые параметры
params = {
    "iterations": 2500,
    "learning_rate": 0.0075,
    "depth": 7,
    "l2_leaf_reg": 5.0,  # регуляризация
    "random_strength": 2.0,
    "bagging_temperature": 0.3,  # лёгкий стохастический бэггинг
    "border_count": 254,  # тоньше квантизация числовых
    "auto_class_weights": "Balanced",
    "verbose": 200,
    "eval_metric": "AUC",
    "random_state": 42,
    "early_stopping_rounds": 200,
}


# ⚙️ настройки кросс-валидации
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

oof_preds = np.zeros(len(train))
oof_true = train[y].values
fold_scores = []
thresholds = []

for fold, (train_idx, val_idx) in enumerate(skf.split(train[X], train[y])):
    X_train, X_val = train.iloc[train_idx][X], train.iloc[val_idx][X]
    y_train, y_val = train.iloc[train_idx][y], train.iloc[val_idx][y]

    model = CatBoostClassifier(**params)
    model.fit(
        X_train,
        y_train,
        cat_features=cat_features,
        eval_set=(X_val, y_val),
        use_best_model=True,
    )

    # Вероятности на валидации
    y_proba = model.predict_proba(X_val)[:, 1]
    oof_preds[val_idx] = y_proba

    # Подбор лучшего порога по F1
    prec, rec, thr = precision_recall_curve(y_val, y_proba)
    f1 = 2 * prec * rec / (prec + rec + 1e-9)
    best_idx = np.argmax(f1[:-1])
    thresholds.append(thr[best_idx])
    fold_scores.append(f1[best_idx])

    print(f"Fold {fold+1}: F1={f1[best_idx]:.4f}, best_thr={thr[best_idx]:.3f}")

# 📊 Усреднённые метрики
best_global_thr = np.median(thresholds)
y_pred_final = (oof_preds >= best_global_thr).astype(int)

f1_final = f1_score(oof_true, y_pred_final)
auc_final = roc_auc_score(oof_true, oof_preds)
acc_final = accuracy_score(oof_true, y_pred_final)
prec_final = precision_score(oof_true, y_pred_final)
rec_final = recall_score(oof_true, y_pred_final)

print("\n📈 Средние результаты по 5 фолдам:")
print(f"ROC AUC: {auc_final:.4f}")
print(f"Accuracy: {acc_final:.4f}")
print(f"Precision: {prec_final:.4f}")
print(f"Recall: {rec_final:.4f}")
print(f"F1-score: {f1_final:.4f}")
print(f"Median threshold: {best_global_thr:.3f}")

# 💡 (опционально) — важность признаков на последнем фолде
fi = pd.DataFrame(
    {"feature": X, "importance": model.get_feature_importance()}
).sort_values("importance", ascending=False)

print("\n🔝 Top 15 важных признаков:")
print(fi.head(15))





params = {
    "iterations": 2500,
    "learning_rate": 0.0075,
    "depth": 7,
    "l2_leaf_reg": 5.0,
    "random_strength": 2.0,
    "bagging_temperature": 0.3,
    "border_count": 254,
    "auto_class_weights": "Balanced",
    "verbose": 200,
    "eval_metric": "AUC",
    "random_state": 42,
    "early_stopping_rounds": 200,
}

model_final = CatBoostClassifier(**params)
model_final.fit(
    train_extra_features[X], train_extra_features[y], cat_features=cat_features
)


df_test = create_features(test)
df_test = df_test.drop(columns=["oral"])


y_proba = model.predict_proba(val[X])[:, 1]
y_pred = (y_proba >= 0.491).astype(int)

y_true = val[y]

acc = accuracy_score(y_true, y_pred)
prec = precision_score(y_true, y_pred)
rec = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)
auc = roc_auc_score(y_true, y_proba)

print("📊 Финальная модель (валидация):")
print(f"Accuracy: {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall: {rec:.4f}")
print(f"F1-score: {f1:.4f}")
print(f"ROC AUC: {auc:.4f}")


y_score_test = model_final.predict_proba(df_test[X])[:, 1]
df_test["smoking"] = (y_score_test > 0.491).astype(int)  # применяем лучший порог
df_test.head(5)


df_test[["id", "smoking"]].to_csv("../data/smoking_v4_sub.csv", index=False)


import sys
get_ipython().getoutput("{sys.executable} -m pip install -U pip")
get_ipython().getoutput("{sys.executable} -m pip install -U codeium-jupyter")


яя
